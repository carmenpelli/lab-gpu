{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454c61ba-825d-4290-b7ed-135a775fe002",
   "metadata": {},
   "source": [
    "### Numpy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8019c56a-e5ce-469b-a406-9c6056ab9c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07 s ± 512 μs per loop (mean ± std. dev. of 2 runs, 1 loop each)\n",
      "Result shape: (7000, 7000)\n",
      "Result type: float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Large matrices (adjust size as needed)\n",
    "n = 7000  # For very large matrices, ensure you have enough RAM\n",
    "A = np.random.rand(n, n).astype(np.float32)\n",
    "B = np.random.rand(n, n).astype(np.float32)\n",
    "\n",
    "C = np.dot(A, B)  # warm-up and Matrix multiplication\n",
    "\n",
    "%timeit -r 2 -o np.dot(A, B)\n",
    "\n",
    "print(f\"Result shape: {C.shape}\")\n",
    "print(f\"Result type: {C.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e220304-9bec-4755-94da-912417b3cbfe",
   "metadata": {},
   "source": [
    "### Multiplicación de matrices con Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af70ad9-d56c-41c1-b9f5-f266758be5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing device:\u001b[39m\u001b[33m\"\u001b[39m, device)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Convert NumPy arrays to PyTorch tensors and move to GPU\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m A_t = torch.from_numpy(\u001b[43mA\u001b[49m).to(device)\n\u001b[32m      9\u001b[39m B_t = torch.from_numpy(B).to(device)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Warm-up\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Select device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors and move to GPU\n",
    "A_t = torch.from_numpy(A).to(device)\n",
    "B_t = torch.from_numpy(B).to(device)\n",
    "\n",
    "# Warm-up\n",
    "C_t = torch.matmul(A_t, B_t)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Measure time\n",
    "%timeit -r 2 -o torch.matmul(A_t, B_t)\n",
    "\n",
    "# Bring result back to CPU \n",
    "C_cpu = C_t.cpu().numpy()\n",
    "\n",
    "print(f\"Result shape: {C_cpu.shape}\")\n",
    "print(f\"Result type: {C_cpu.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf5d5b-3596-4237-b834-8364e71bea9a",
   "metadata": {},
   "source": [
    "### Cálculo de pi con Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4355c834-c6d7-49fe-88a5-16f31bc6f204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce GTX 1080 which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.13/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/usr/local/lib/python3.13/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "NVIDIA GeForce GTX 1080 with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce GTX 1080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing device:\u001b[39m\u001b[33m\"\u001b[39m, device)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Generar puntos aleatorios en GPU\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m y = torch.rand(N, device=device)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_pi_torch\u001b[39m(x, y):\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Número de puntos\n",
    "N = 10_000_000\n",
    "\n",
    "# Selección de dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Generar puntos aleatorios en GPU\n",
    "x = torch.rand(N, device=device)\n",
    "y = torch.rand(N, device=device)\n",
    "\n",
    "def calc_pi_torch(x, y):\n",
    "    dist_sq = x*x + y*y\n",
    "    M = torch.sum(dist_sq < 1.0)\n",
    "    return 4.0 * M.item() / x.numel()\n",
    "\n",
    "# Warm-up\n",
    "pi = calc_pi_torch(x, y)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Medida de tiempo\n",
    "%timeit -r 3 calc_pi_torch(x, y)\n",
    "\n",
    "print(\"\\n \\t Computing pi with PyTorch: \\n\")\n",
    "print(f\"\\t For {N} trials, pi = {pi}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ab44f-0281-4b98-9626-26901c3df085",
   "metadata": {},
   "source": [
    "### Resultados obtenidos tras ejecutar en la cola bohr-gpu\n",
    "\n",
    "**Numpy code**\n",
    "\n",
    "622 ms ± 1.75 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n",
    "\n",
    "Result shape: (7000, 7000)\n",
    "\n",
    "Result type: float32\n",
    "\n",
    "**Pytorch**\n",
    "\n",
    "Using device: cuda\n",
    "\n",
    "53.4 ms ± 1.25 ms per loop (mean ± std. dev. of 2 runs, 1,000 loops each)\n",
    "\n",
    "Result shape: (7000, 7000)\n",
    "\n",
    "Result type: float32\n",
    "\n",
    "**Pi con pytorch**\n",
    "\n",
    "Using device: cuda\n",
    "\n",
    "944 μs ± 107 ns per loop (mean ± std. dev. of 3 runs, 1,000 loops each)\n",
    "\n",
    " \t Computing pi with PyTorch: \n",
    "\n",
    "\t For 10000000 trials, pi = 3.141384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617de79b-2f79-4b5c-aaf5-efef4bfabf02",
   "metadata": {},
   "source": [
    "## Actividad extra: PyTorch en GPU\n",
    "\n",
    "En la multiplicación de matrices se observa una aceleración notable respecto a NumPy, reduciendo el tiempo de ejecución de varios cientos de milisegundos a unas pocas decenas, lo que confirma la eficiencia de la GPU para operaciones matriciales densas.\n",
    "\n",
    "Asimismo, el cálculo de π mediante el método de Monte Carlo muestra tiempos muy reducidos cuando toda la computación se realiza en GPU, devolviendo únicamente un escalar a la CPU. El tiempo medido con PyTorch corresponde únicamente al cálculo en GPU, excluyendo la generación de los datos y la inicialización, tras realizar un warm-up previo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
