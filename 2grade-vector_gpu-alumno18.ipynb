{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3431319e-2e6b-43c1-a2f6-61ccba592c21",
   "metadata": {},
   "source": [
    "## Evaluating a vectorial function on CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d142183-b6bb-42e7-ba9d-35bf0f91dc0c",
   "metadata": {},
   "source": [
    "### CPU: plain and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92dd336-9997-4323-80e4-f285e9cc2db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "8.55 ms ± 199 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "18.9 ms ± 22.8 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "18.9 ms ± 207 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit, jit\n",
    "\n",
    "# Python plain implementation w/ numba \n",
    "@njit\n",
    "def grade2_vector(x, y, a, b, c):\n",
    "    z = np.zeros(x.size)\n",
    "    for i in range(x.size):\n",
    "        z[i] = a*x[i]*x[i] + b*y[i] + c\n",
    "    return z\n",
    "\n",
    "# Numpy ufunc\n",
    "def grade2_ufunc(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "# size of the vectors\n",
    "size = 5_000_000\n",
    "\n",
    "# allocating and populating the vectors\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "c_cpu = np.zeros(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "# Printing input values\n",
    "#print(a_cpu)\n",
    "#print(b_cpu)\n",
    "# Random function in Numpy always use float64\n",
    "print(a_cpu.dtype)\n",
    "\n",
    "c_cpu = grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "\n",
    "# Evaluating the time\n",
    "\n",
    "# Numba Python: huge improvement, better that numpy code\n",
    "%timeit -n 5 -r 2 grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "# w/ a numpy ufunc manually coded\n",
    "%timeit -n 5 -r 2 grade2_ufunc(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "# using the general numpy ufunc \n",
    "%timeit -n 5 -r 2 a*a_cpu**2 + b*b_cpu + c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79807696-7218-46dc-bb69-2452f5dbda03",
   "metadata": {},
   "source": [
    "### Librería Cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7c48fa-d484-4827-82d5-8cbff2bb9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6903083-d88b-4d9e-b9f1-154e9d57c115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy (with copy) time [ms]: 15.598591804504395\n"
     ]
    }
   ],
   "source": [
    "# Copy arrays from CPU to GPU\n",
    "a_gpu = cp.asarray(a_cpu)\n",
    "b_gpu = cp.asarray(b_cpu)\n",
    "\n",
    "# Allocate output on GPU\n",
    "c_gpu = cp.zeros_like(a_gpu)\n",
    "\n",
    "# Define operation using CuPy ufuncs\n",
    "def grade2_cupy(x, y, a, b, c):\n",
    "    return a * x**2 + b * y + c\n",
    "\n",
    "# Warm-up\n",
    "grade2_cupy(a_gpu, b_gpu, a, b, c)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "\n",
    "# Benchmark INCLUDING data copy (correct)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "cp._default_memory_pool.free_all_blocks()\n",
    "\n",
    "start = cp.cuda.Event()\n",
    "end = cp.cuda.Event()\n",
    "\n",
    "start.record()\n",
    "a_gpu = cp.asarray(a_cpu)\n",
    "b_gpu = cp.asarray(b_cpu)\n",
    "c_gpu = grade2_cupy(a_gpu, b_gpu, a, b, c)\n",
    "end.record()\n",
    "\n",
    "end.synchronize()\n",
    "print(\"CuPy (with copy) time [ms]:\", \n",
    "      cp.cuda.get_elapsed_time(start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6120633-a7b1-4ba1-b43d-fab5c3a553e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy (no copy) time [ms]: 4.786880016326904\n"
     ]
    }
   ],
   "source": [
    "# Create arrays directly on GPU\n",
    "a_gpu = cp.random.rand(size, dtype=cp.float64)\n",
    "b_gpu = cp.random.rand(size, dtype=cp.float64)\n",
    "\n",
    "# Warm-up\n",
    "grade2_cupy(a_gpu, b_gpu, a, b, c)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "\n",
    "# Benchmark without data copy\n",
    "start.record()\n",
    "c_gpu = grade2_cupy(a_gpu, b_gpu, a, b, c)\n",
    "end.record()\n",
    "\n",
    "end.synchronize()\n",
    "print(\"CuPy (no copy) time [ms]:\", \n",
    "      cp.cuda.get_elapsed_time(start, end))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819d9cf-1bfd-473a-8a44-3f351e3940c0",
   "metadata": {},
   "source": [
    "### Librería Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef1fa656-d6bd-48de-b554-df9aa520f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5cb0c88-2759-4b2c-9194-fb9e0bb01157",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(\n",
    "    ['float64(float64, float64, float64, float64, float64)'],\n",
    "    target='cuda'\n",
    ")\n",
    "def grade2_numba_gpu(x, y, a, b, c):\n",
    "    return a*x*x + b*y + c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78a9a179-eb95-4f8a-b1e6-9d6626448a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba GPU (with copy) time [ms]: 17.636383056640625\n"
     ]
    }
   ],
   "source": [
    "# Warm-up\n",
    "grade2_numba_gpu(a_cpu[:10], b_cpu[:10], a, b, c)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "\n",
    "# Benchmark with implicit copy\n",
    "start = cp.cuda.Event()\n",
    "end = cp.cuda.Event()\n",
    "\n",
    "start.record()\n",
    "c_gpu = grade2_numba_gpu(a_cpu, b_cpu, a, b, c)\n",
    "end.record()\n",
    "\n",
    "end.synchronize()\n",
    "print(\"Numba GPU (with copy) time [ms]:\",\n",
    "      cp.cuda.get_elapsed_time(start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77c7b254-16e6-470b-af7f-b62dcbc7100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba GPU (no copy) time [ms]: 1.6814080476760864\n"
     ]
    }
   ],
   "source": [
    "# Manual copy to GPU\n",
    "a_gpu = cp.asarray(a_cpu)\n",
    "b_gpu = cp.asarray(b_cpu)\n",
    "\n",
    "# Warm-up\n",
    "grade2_numba_gpu(a_gpu, b_gpu, a, b, c)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "\n",
    "# Benchmark without copy\n",
    "start = cp.cuda.Event()\n",
    "end = cp.cuda.Event()\n",
    "\n",
    "start.record()\n",
    "c_gpu = grade2_numba_gpu(a_gpu, b_gpu, a, b, c)\n",
    "end.record()\n",
    "\n",
    "end.synchronize()\n",
    "print(\"Numba GPU (no copy) time [ms]:\", \n",
    "      cp.cuda.get_elapsed_time(start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb5e8f-4e43-403a-9be0-c7b120de41dd",
   "metadata": {},
   "source": [
    "### Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980093f2-89d7-4aef-a102-9f741e858b14",
   "metadata": {},
   "source": [
    "Los resultados obtenidos muestran claramente el impacto del uso de la GPU y, especialmente, del coste asociado a la copia de datos entre CPU y GPU.\n",
    "\n",
    "En el caso de CuPy, cuando se incluyen las copias de datos desde la CPU a la GPU, el tiempo de ejecución aumenta de forma notable (≈15.6 ms). Sin embargo, cuando los datos se crean directamente en la GPU y se evita la transferencia, el tiempo se reduce significativamente (≈4.8 ms). Esto indica que el cálculo en sí es rápido, pero la transferencia de datos puede convertirse en el principal cuello de botella.\n",
    "\n",
    "Para Numba en GPU, el comportamiento es similar, aunque aún más acusado. Con copia implícita de datos, el tiempo es comparable al de CuPy con copia (≈17.6 ms). No obstante, cuando los datos ya residen en la GPU, el tiempo de ejecución se reduce drásticamente hasta valores muy bajos (≈1.7 ms), siendo la opción más rápida de todas las probadas.\n",
    "\n",
    "En conjunto, estos resultados ponen de manifiesto que el uso de la GPU ofrece mejoras significativas de rendimiento, pero solo cuando se minimiza el coste de transferencia de datos. El mayor beneficio se obtiene cuando los datos permanecen en la GPU durante todo el cálculo, lo que resalta la importancia de una gestión eficiente de la memoria en aplicaciones aceleradas por GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
